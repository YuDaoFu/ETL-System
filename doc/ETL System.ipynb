{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "163173d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e516dbe4",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c22561",
   "metadata": {},
   "source": [
    "**Table 1**\n",
    "\n",
    "**Title:** data_v2\n",
    "\n",
    "**Fields:** company_id, ticker, date, volume, price, ref_num, source, name\n",
    "\n",
    "\n",
    "**Table 2**\n",
    "\n",
    "**Title:** reference\n",
    "\n",
    "**Fields:** id, ticker, name\n",
    "\n",
    "\n",
    "\n",
    "**Constraints**:  \n",
    "- Every row in **data_v2** must have one and only one corresponding element from **referrence**\n",
    "- Every row from **referrence** can correspond to zero or many elements in **data_v2**.\n",
    "- Volumn should be non-negative\n",
    "- Price should be non-negative\n",
    "- Each id, ticker and name from **reference** tbale should be unique\n",
    "\n",
    "---\n",
    "Here I used Crow's Foot Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821c4c5",
   "metadata": {},
   "source": [
    "![Schema.jpg](Schema.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872ba42",
   "metadata": {},
   "source": [
    "### Side Note:\n",
    "For **reference** table:\n",
    "- Here I assume (id, ticker) forms the prime key for the **reference** table as the name is not atomic.\n",
    "- As the SEC guidelines simply require that the choice of a ticker symbol shouldbe original (i.e. not replicate another company's stock ticker symbol) thus I will drop the second row with duplicated ticker \n",
    "- For rows with duplicated name, I will drop the row without a correct corresponding ticker - name\n",
    "\n",
    "\n",
    "For **check_validate** function:\n",
    "- As missing numerical values can be filled by data scientist or our client according to their wills (average, mode, etc), I didn't count missing values as failed rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d7c33",
   "metadata": {},
   "source": [
    "## Loads Zip Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72717c",
   "metadata": {},
   "source": [
    "### Side Note:\n",
    "I have manually check the stock price for both repaeted tickers towards the corresponding datetime and the price did not macthing to the real ones; thus I assume the data is synthetic and I will drop the second row for the duplicated ticker. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93b520",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "for item in list(df2[df2['name'].duplicated()==True]['name']):\n",
    "    print(df2[df2['name']==item],'\\n')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18fc8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_v2(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def load_reference(path: str)-> pd.DataFrame:\n",
    "    \n",
    "    # perform data cleaning for reference table to ensure there is no duplicated ticker\n",
    "    # and each ticker is corresponding to the right name. Also rename id to company_id \n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df.rename(columns={\"id\": \"company_id\", \"ticker\": \"ticker\", \"name\": \"name\"},inplace = True)\n",
    "    \n",
    "    # drop duplicated ticker\n",
    "    df = df[df['ticker'].duplicated()!=True]\n",
    "    \n",
    "    # drop rows with incorrect ticker - name pair\n",
    "    # error_list refer to the test code above where I manually checked each ticker\n",
    "    # corresponding to the right name \n",
    "    \n",
    "    error_list = [431,245,691,792,820,814,906]\n",
    "    \n",
    "    df.drop(index = error_list,inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673f5c8",
   "metadata": {},
   "source": [
    "## Check Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673b514a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validate(df: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Return DataFrame where error rows have been dropped, and error DataFrame which\n",
    "    # contains error info\n",
    "    \n",
    "    # part a) \n",
    "    \n",
    "\n",
    "    # Error output dataframe should have columns: check_performed, line_number, \n",
    "    # field, expected_value, actual_value\n",
    "    \n",
    "    # Based on the constarints: We should have company_id refer to id in reference, \n",
    "    # volume non-negative, price non-negative\n",
    "\n",
    "    \n",
    "    # check if company_id in reference\n",
    "    id_list = list(df2['company_id'])\n",
    "\n",
    "    reference_error = df[df['company_id'].isin(id_list)!=True]\n",
    "    reference_error['check_performed'] = 'Schema Check'\n",
    "    reference_error['line_number'] = reference_error.index\n",
    "    reference_error['field'] = 'company_id'\n",
    "    reference_error['expected_value'] = 'company_id refer to id in reference'\n",
    "    reference_error['actual_value'] = reference_error['company_id']\n",
    "    \n",
    "    # Check volume and price if there was any negative values\n",
    "    volume_error = df[df.eval('volume < 0')]\n",
    "    volume_error['check_performed'] = 'Negative Value'\n",
    "    volume_error['line_number'] = volume_error.index\n",
    "    volume_error['field'] = 'volume'\n",
    "    volume_error['expected_value'] = 'volume >= 0'\n",
    "    volume_error['actual_value'] = volume_error['volume']\n",
    "    \n",
    "    price_error = df[df.eval('price < 0')]\n",
    "    price_error['check_performed'] = 'Negative Value'\n",
    "    price_error['line_number'] = price_error.index\n",
    "    price_error['field'] = 'price'\n",
    "    price_error['expected_value'] = 'price >= 0'\n",
    "    price_error['actual_value'] = price_error['price']\n",
    "\n",
    "    \n",
    "    ## check duplicated rows if have the same date and ticker\n",
    "    \n",
    "    temp = df[df['company_id'].isin(id_list)] # get rows in the reference \n",
    "    duplicated_error = temp[temp.duplicated()]\n",
    "    duplicated_error['check_performed'] = 'Duplicated Rows'\n",
    "    duplicated_error['line_number'] = duplicated_error.index\n",
    "    duplicated_error['field'] = 'Ticker and Date'\n",
    "    duplicated_error['expected_value'] = 'Non Duplicates'\n",
    "    duplicated_error['actual_value'] = 'Duplicates'\n",
    "    \n",
    "    # concate into one DataFrame\n",
    "    \n",
    "    error_df = pd.concat([reference_error,volume_error,price_error,duplicated_error])\n",
    "    error_df = error_df[['check_performed','line_number','field','expected_value','actual_value']]\n",
    "    error_df.reset_index(inplace = True,drop = True)\n",
    "    \n",
    "    # part b) Remove any failed rows\n",
    "    \n",
    "#     new_df = temp[temp[['date','ticker']].duplicated()!=True]\n",
    "    temp.drop_duplicates(inplace = True)\n",
    "    \n",
    "    return temp, error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7a2e95",
   "metadata": {},
   "source": [
    "## Data Transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "426b74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(df: pd.DataFrame,df2: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # fill the name column (data_v2) \n",
    "    # referring to name in reference and unnecessary columns\n",
    "    \n",
    "    new_df = pd.merge(df, df2, how=\"left\", on=[\"company_id\"])\n",
    "    new_df = new_df[['company_id','ticker_y','date', 'volume', 'price', 'ref_num','name_y']]\n",
    "    new_df.rename({'ticker_y':'ticker','name_y':'name'},inplace = True,axis='columns')\n",
    "    \n",
    "    \n",
    "    # replace NaN values for ticker column (data_v2) \n",
    "    # referring to ticker in reference\n",
    "    ticker_missing = new_df[new_df['ticker'].isnull()]\n",
    "    index = ticker_missing.index\n",
    "    result = pd.merge(ticker_missing, df2, how=\"left\", on=[\"company_id\"])\n",
    "    result = result[['company_id','ticker_y','date', 'volume', 'price', 'ref_num'\n",
    "                     ,'name_y']].set_index(index)\n",
    "\n",
    "    for i in list(index):\n",
    "        new_df.loc[i,list(new_df.keys())]=result.loc[i].values\n",
    "    \n",
    "    \n",
    "    # convert str time to datetime time\n",
    "    new_df['date'] = pd.to_datetime(new_df['date'])\n",
    "    \n",
    "    \n",
    "    # add new column total_value to each row\n",
    "    new_df['total_value'] = None\n",
    "    total_value = new_df['volume']*new_df['price']\n",
    "    new_df['total_value'] = total_value\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865cbfdf",
   "metadata": {},
   "source": [
    "## Export Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "623d3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_clean_data(df: pd.DataFrame,output_path: str):\n",
    "    return df.to_csv(output_path+'data_clean.csv',index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c836025b",
   "metadata": {},
   "source": [
    "## Report Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854389be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_details(df: pd.DataFrame, old_df: pd.DataFrame, error_df:pd.DataFrame) -> pd.Series:\n",
    "    \n",
    "    # get the columns with nan values\n",
    "    \n",
    "    val = df.isna().sum().sort_values(ascending=False)\n",
    "    print('columns with nan values:\\n',val)\n",
    "    \n",
    "    # input row, output row, and error row\n",
    "\n",
    "    input_row = old_df.shape[0]\n",
    "    output_row = df.shape[0]\n",
    "    error_row = error_df.shape[0]\n",
    "    \n",
    "    print(f'\\nThe number of input rows is {input_row:,}\\nThe number of output rows is {output_row:,}\\nThe number of error rows is {error_row:,}')\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a2f351",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f5a550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time used 14.818 in sceonds \n",
      "\n",
      "columns with nan values:\n",
      " total_value    73\n",
      "price          48\n",
      "volume         25\n",
      "company_id      0\n",
      "ticker          0\n",
      "date            0\n",
      "ref_num         0\n",
      "name            0\n",
      "dtype: int64\n",
      "\n",
      "The number of input rows is 1,461,020\n",
      "The number of output rows is 1,440,546\n",
      "The number of error rows is 20,474\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' or '__file__' in globals():\n",
    "    start = time.time()\n",
    "    path_data = 's3://alpharoc-dev/data_challenge/etl/data_v2.csv.gz'\n",
    "    path_reference = 's3://alpharoc-dev/data_challenge/etl/reference.csv.gz'\n",
    "    output_path = '/home/fu_wang/'\n",
    "    data_v2 = load_data_v2(path_data)\n",
    "    reference = load_reference(path_reference)\n",
    "    new_data_v2, error_df = check_validate(data_v2,reference)\n",
    "    clean_data_v2 = data_transform(new_data_v2,reference)\n",
    "    export_clean_data(clean_data_v2,output_path)\n",
    "    end = time.time()\n",
    "    print(f'Total time used {round(end - start,3)} in sceonds','\\n')\n",
    "    report_details(clean_data_v2,data_v2,error_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52664d3",
   "metadata": {},
   "source": [
    "### Comments:\n",
    "1. The numerical category has extreme values towards on the quantiles $[0,0.05]$  and $[0.95,1]$, and I suggest data scientist or clients to pay extra attention to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df8da177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10    109.96\n",
       "0.99    199.01\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_v2['price'].quantile([0.1, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6fef18b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001           80.0\n",
       "1.0000    999999999.0\n",
       "Name: volume, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data_v2['volume'].quantile([0.0001, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9772b2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9-Default (python3)",
   "language": "python",
   "name": "conda-env-Python3.9-Default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
